# üöÄ Guide Pratique DevOps - Kubernetes R√©silience et Monitoring

---

## üìã **PARTIE 1 : Topology Spread Constraints**

## üîß √âtape 1 : Pr√©paration du Cluster

### Cr√©er un cluster multi-n≈ìuds
```bash
# Supprimer un cluster existant si n√©cessaire
minikube delete

# Cr√©er un cluster avec 2 n≈ìuds
minikube start --nodes 2 --cpus 3 --memory 4096

# V√©rifier les n≈ìuds
kubectl get nodes
```

**R√©sultat attendu :**
```
NAME           STATUS   ROLES           AGE   VERSION
minikube       Ready    control-plane   10m   v1.34.0
minikube-m02   Ready    <none>          10m   v1.34.0
```

---

## üì¶ √âtape 2 : D√©ploiement de l'Application R√©siliente

### D√©ployer l'application
```bash
# Appliquer le deployment avec Topology Spread Constraints
kubectl apply -f resilient-app-deployment.yaml

# Cr√©er le service d'exposition
kubectl apply -f resilient-app-service.yaml
```

### V√©rifier le d√©ploiement
```bash
# Voir tous les pods avec leur r√©partition
kubectl get pods -o wide | grep resilient

# V√©rifier les services
kubectl get services | grep resilient

# Voir le statut du deployment
kubectl get deployment resilient-app
```

**R√©sultat attendu :**
- 4 pods d√©ploy√©s
- 2 pods sur chaque n≈ìud (r√©partition √©quitable)
- Services cr√©√©s et accessibles

---

## üß™ √âtape 3 : Tests de Fonctionnement

### Test de base de l'application
```bash
# Tester l'application depuis un pod
kubectl exec -it $(kubectl get pods | grep resilient | head -1 | awk '{print $1}') -- curl localhost

# Voir les d√©tails de configuration
kubectl get deployment resilient-app -o yaml | grep -A 10 -B 5 topologySpreadConstraints
```

### V√©rifier la r√©partition des pods
```bash
# Voir la r√©partition d√©taill√©e
kubectl get pods -l app=resilient-app -o wide

# Compter les pods par n≈ìud
echo "Pods sur minikube:"
kubectl get pods -o wide | grep resilient | grep minikube | grep -v minikube-m02 | wc -l

echo "Pods sur minikube-m02:"
kubectl get pods -o wide | grep resilient | grep minikube-m02 | wc -l
```

---

## üî• √âtape 4 : Tests de R√©silience

### Test 1 - Simulation de panne d'un n≈ìud
```bash
echo "=== AVANT - R√©partition des pods ==="
kubectl get pods -o wide | grep resilient

echo "=== SIMULATION PANNE - Drainage du n≈ìud minikube-m02 ==="
kubectl drain minikube-m02 --ignore-daemonsets --delete-emptydir-data --force

echo "=== PENDANT - √âtat des pods apr√®s drainage ==="
kubectl get pods -o wide | grep resilient

echo "=== ANALYSE - Pourquoi certains pods sont Pending ==="
kubectl describe pod $(kubectl get pods | grep Pending | head -1 | awk '{print $1}') | grep -A 5 "Events:"
```

### Test 2 - Remise en service
```bash
echo "=== REMISE EN SERVICE du n≈ìud ==="
kubectl uncordon minikube-m02

echo "=== Attendre la redistribution (5 secondes) ==="
sleep 5

echo "=== APR√àS - Nouvelle r√©partition ==="
kubectl get pods -o wide | grep resilient
```

### Test 3 - V√©rification fonctionnelle apr√®s tests
```bash
echo "=== TEST FONCTIONNEL - L'application fonctionne-t-elle ? ==="
kubectl exec -it $(kubectl get pods | grep resilient | grep Running | head -1 | awk '{print $1}') -- curl localhost | head -3
```

---

## üìä √âtape 5 : Validation et Nettoyage

### Valider que tous les crit√®res sont respect√©s
```bash
echo "=== VALIDATION FINALE ==="

echo "1. Nombre de n≈ìuds disponibles:"
kubectl get nodes | grep Ready | wc -l

echo "2. R√©partition √©quitable des pods:"
kubectl get pods -l app=resilient-app -o wide

echo "3. Status du deployment:"
kubectl get deployment resilient-app

echo "4. Services accessibles:"
kubectl get services | grep resilient

echo "5. Test application:"
kubectl exec -it $(kubectl get pods | grep resilient | grep Running | head -1 | awk '{print $1}') -- curl -s localhost | grep -o "<title>.*</title>"
```

### Nettoyage (optionnel)
```bash
# Supprimer l'application
kubectl delete -f resilient-app-deployment.yaml
kubectl delete -f resilient-app-service.yaml

# Ou supprimer tout par label
kubectl delete all -l app=resilient-app
```

---

## üîç Commandes de Diagnostic

### Debug en cas de probl√®me
```bash
# Voir tous les √©v√©nements r√©cents
kubectl get events --all-namespaces --sort-by='.lastTimestamp' | tail -20

# Diagnostiquer un pod en erreur
kubectl describe pod <nom-du-pod>

# Voir les logs d'un pod
kubectl logs <nom-du-pod>

# V√©rifier l'√©tat des n≈ìuds
kubectl describe nodes

# Voir les contraintes de topologie appliqu√©es
kubectl get deployment resilient-app -o jsonpath='{.spec.template.spec.topologySpreadConstraints}' | jq .
```

### Commandes de surveillance
```bash
# Surveiller les pods en temps r√©el
kubectl get pods -w

# Voir l'utilisation des ressources
kubectl top nodes
kubectl top pods

# V√©rifier les services
kubectl get endpoints
kubectl describe service resilient-app
```

---

## üìã **PARTIE 2 : Stack Monitoring Prometheus/Grafana**

## üîß √âtape 6 : Installation de la Stack de Monitoring

### Ajouter le repository Helm Prometheus
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

### D√©ployer la stack kube-prometheus-stack
```bash
helm install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace
```

### V√©rifier le d√©ploiement
```bash
kubectl get pods -n monitoring
kubectl get services -n monitoring
```

---

## üîß √âtape 7 : Configuration des Acc√®s Web

### Acc√©der √† Grafana
```bash
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 &
```

### Acc√©der √† Prometheus
```bash
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 &
```

### R√©cup√©rer les credentials Grafana
```bash
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
```

---

## üîß √âtape 8 : Validation du Monitoring

### Tester l'acc√®s aux interfaces
```bash
curl http://localhost:3000/api/health
curl http://localhost:9090/-/healthy
```

### V√©rifier les dashboards Kubernetes
```bash
kubectl get configmaps -n monitoring | grep dashboard
```

---

## üìã **PARTIE 3 : Tests de Charge avec Ingress**

## üîß √âtape 9 : Configuration Ingress

### Activer l'addon ingress minikube
```bash
minikube addons enable ingress
kubectl get pods -n ingress-nginx
```

### Cr√©er l'Ingress pour guestbook
```bash
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: guestbook-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: guestbook.fbi.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: guestbook-service
            port:
              number: 80
EOF
```

### D√©marrer minikube tunnel
```bash
minikube tunnel &
```

---

## üîß √âtape 10 : Tests de Charge k6

### Tester l'acc√®s via Ingress
```bash
echo "$(minikube ip) guestbook.fbi.com" | sudo tee -a /etc/hosts
curl http://guestbook.fbi.com
```

### Cr√©er le script de test k6
```bash
cat > load-test.js <<EOF
import http from 'k6/http';
import { check } from 'k6';

export let options = {
  stages: [
    { duration: '2m', target: 10 },
    { duration: '5m', target: 50 },
    { duration: '2m', target: 0 },
  ],
};

export default function () {
  let response = http.get('http://guestbook.fbi.com');
  check(response, {
    'status is 200': (r) => r.status === 200,
  });
}
EOF
```

### Lancer les tests k6
```bash
k6 run load-test.js
```

---

## üîß √âtape 11 : Observation des M√©triques

### Surveiller pendant les tests
```bash
kubectl get pods -w
kubectl top nodes
kubectl top pods
```

### V√©rifier les m√©triques dans Grafana
```bash
echo "Acc√©dez √† http://localhost:3000"
echo "Username: admin"
echo "Password: $(kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 --decode)"
```

üöÄ **J'ai maintenant un monitoring complet avec tests de performance valid√©s !**

---

## üìã **PARTIE 4 : Dashboard Custom pour Application**

## üîß √âtape 12 : Exploration Endpoint M√©triques

### Explorer l'endpoint /info du guestbook
```bash
kubectl port-forward service/guestbook-service 8080:80 &
curl http://localhost:8080/info
```

### V√©rifier ce que Prometheus scrappe
```bash
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 &
curl "http://localhost:9090/api/v1/label/__name__/values" | jq
```

---

## üîß √âtape 13 : Cr√©ation Dashboard Grafana

### Cr√©er un nouveau dashboard dans Grafana
```bash
echo "Acc√©dez √† http://localhost:3000"
echo "Cliquez sur '+' > Dashboard > Add Panel"
```

### Exemples de requ√™tes PromQL pour panels
```bash
# Taux de requ√™tes par seconde
rate(http_requests_total[5m])

# Latence moyenne
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Erreurs par minute
increase(http_requests_total{status=~"5.."}[1m])
```

---

## üîß √âtape 14 : Configuration Alertes

### Cr√©er des alertes sur m√©triques critiques
```bash
# Dans Grafana: Alerting > Alert Rules > New Rule
# Condition: http_requests_total rate > 100
# Evaluation: every 10s for 30s
```

---

## üìã **PARTIE 5 : Chaos Engineering avec Chaos Mesh**

## üîß √âtape 15 : Installation Chaos Mesh

### Ajouter le repository Helm Chaos Mesh
```bash
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm repo update
```

### D√©ployer Chaos Mesh
```bash
helm install chaos-mesh chaos-mesh/chaos-mesh --namespace chaos-mesh --create-namespace --set dashboard.create=true
```

### V√©rifier l'installation
```bash
kubectl get pods -n chaos-mesh
kubectl get crd | grep chaos
```

---

## üîß √âtape 16 : Acc√®s Dashboard Chaos Mesh

### Acc√©der au dashboard
```bash
kubectl port-forward -n chaos-mesh svc/chaos-dashboard 2333:2333 &
echo "Dashboard accessible: http://localhost:2333"
```

---

## üîß √âtape 17 : Exp√©rience Pod-Kill

### Cr√©er l'exp√©rience pod-kill
```bash
kubectl apply -f - <<EOF
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: guestbook-pod-kill
  namespace: default
spec:
  action: pod-kill
  mode: one
  duration: "30s"
  selector:
    labelSelectors:
      app: guestbook
  scheduler:
    cron: "@every 2m"
EOF
```

### Surveiller l'impact
```bash
kubectl get pods -w
kubectl logs -f deployment/guestbook
```

---

## üìã **PARTIE 6 : GitHub Actions Runner Self-Hosted**

## üîß √âtape 18 : Pr√©paration Token GitHub

### Cr√©er un token dans GitHub
```bash
echo "1. Allez dans Settings > Actions > Runners"
echo "2. Cliquez 'New self-hosted runner'"
echo "3. Copiez le token affich√©"
```

### Cr√©er le secret Kubernetes
```bash
kubectl create secret generic github-runner-token --from-literal=token=YOUR_TOKEN_HERE
```

---

## üîß √âtape 19 : D√©ploiement Runner

### D√©ployer le runner comme pod
```bash
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: github-runner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: github-runner
  template:
    metadata:
      labels:
        app: github-runner
    spec:
      containers:
      - name: runner
        image: sumologic/github-runner:latest
        env:
        - name: GITHUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: github-runner-token
              key: token
        - name: GITHUB_OWNER
          value: "BrahimHmitti"
        - name: GITHUB_REPOSITORY
          value: "DEVOPS-jour4"
        volumeMounts:
        - name: docker-sock
          mountPath: /var/run/docker.sock
      volumes:
      - name: docker-sock
        hostPath:
          path: /var/run/docker.sock
EOF
```

---

## üîß √âtape 20 : Test du Runner

### V√©rifier l'enregistrement
```bash
kubectl logs deployment/github-runner
kubectl get pods -l app=github-runner
```

### Cr√©er un workflow de test
```bash
mkdir -p .github/workflows
cat > .github/workflows/test-runner.yml <<EOF
name: Test Self-Hosted Runner
on: [push]
jobs:
  test:
    runs-on: self-hosted
    steps:
    - uses: actions/checkout@v3
    - name: Test
      run: echo "Runner fonctionne dans Kubernetes!"
EOF
```

### Pousser et v√©rifier
```bash
git add .github/
git commit -m "Test runner self-hosted"
git push
```

üöÄ **J'ai maintenant un environnement DevOps complet : monitoring + chaos engineering + CI/CD !**
